{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyperparameters\n",
    "    * Configuration settings for the model and training process.\n",
    "    * Starting with smaller values is recommended for initial testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000       # Size of the vocabulary (number of unique tokens)\n",
    "embedding_dim = 64      # Dimension of token embeddings\n",
    "hidden_dim = 128        # Dimension of the hidden layer in the feed-forward network\n",
    "num_heads = 2           # Number of attention heads in MultiHeadAttention\n",
    "num_layers = 2          # Number of stacked Transformer Blocks\n",
    "sequence_length = 32    # Maximum length of input/output sequences\n",
    "batch_size = 32         # Number of sequences processed in parallel (Note: Not fully utilized in the current simplified loop)\n",
    "learning_rate = 0.001   # Step size for the optimizer\n",
    "num_epochs = 10         # Number of complete passes through the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Transformer Block Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block of the Transformer encoder architecture.\n",
    "\n",
    "    Contains a Multi-Head Self-Attention layer followed by a Position-wise\n",
    "    Feed-Forward Network. Includes residual connections and layer normalization\n",
    "    after each sub-layer.\n",
    "\n",
    "    Args:\n",
    "        embedding_dim (int): The dimension of the input embeddings.\n",
    "        hidden_dim (int): The dimension of the hidden layer in the feed-forward network.\n",
    "        num_heads (int): The number of attention heads for MultiHeadAttention.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # Multi-Head Self-Attention layer\n",
    "        # Takes query, key, value inputs (all are 'x' for self-attention)\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True) # Assume batch_first=True based on typical usage\n",
    "        # Position-wise Feed-Forward Network (2 linear layers with ReLU)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "        # Layer Normalization applied after attention and feed-forward\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        # TODO: Add Dropout layers for regularization if needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer Block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        # --- Self-Attention Sub-layer ---\n",
    "        # Calculate attention output. Note: MHA returns (attn_output, attn_weights)\n",
    "        # We use x as query, key, and value for self-attention.\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        # Residual Connection (Add) & Layer Normalization (Norm)\n",
    "        x = self.norm1(x + attention_output)\n",
    "\n",
    "        # --- Feed-Forward Sub-layer ---\n",
    "        # Pass through linear layers with ReLU activation\n",
    "        linear_output = self.linear2(torch.relu(self.linear1(x)))\n",
    "        # Residual Connection (Add) & Layer Normalization (Norm)\n",
    "        x = self.norm2(x + linear_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Minimal Transformer Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal Transformer model for sequence-to-sequence tasks like language modeling.\n",
    "\n",
    "    Stacks multiple TransformerBlocks on top of an embedding layer and adds a final\n",
    "    linear layer to predict vocabulary logits. Includes a simplified positional encoding.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embedding_dim (int): Dimension of token embeddings.\n",
    "        hidden_dim (int): Dimension of the hidden layer in TransformerBlock feed-forward networks.\n",
    "        num_heads (int): Number of attention heads in TransformerBlocks.\n",
    "        num_layers (int): Number of TransformerBlocks to stack.\n",
    "        sequence_length (int): Maximum length of input sequences for positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_heads, num_layers, sequence_length):\n",
    "        super().__init__()\n",
    "        # Embedding layer: Maps vocabulary indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # --- Positional Encoding ---\n",
    "        # Placeholder: Uses zeros. A proper implementation (sinusoidal or learned) is crucial\n",
    "        # for the model to understand token order. It should be added to the embeddings.\n",
    "        # Note: This tensor is not registered as a parameter or buffer by default.\n",
    "        # Consider registering as buffer: self.register_buffer('positional_encoding', ...)\n",
    "        # Shape: (sequence_length, embedding_dim)\n",
    "        self.positional_encoding = torch.zeros(sequence_length, embedding_dim)\n",
    "        # TODO: Implement proper positional encoding.\n",
    "\n",
    "        # Stack of Transformer Blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embedding_dim, hidden_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final Linear Layer: Maps the Transformer output back to vocabulary logits\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Minimal Transformer model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token indices, shape (batch_size, sequence_length)\n",
    "                              or just (sequence_length) if batch_size is 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of logits, shape (batch_size, sequence_length, vocab_size)\n",
    "                          or (sequence_length, vocab_size).\n",
    "        \"\"\"\n",
    "        # 1. Get Token Embeddings\n",
    "        # Shape: (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 2. Add Positional Encoding\n",
    "        # Broadcasting is needed here. Assumes x is (batch_size, seq_len, embed_dim)\n",
    "        # and positional_encoding is (seq_len, embed_dim).\n",
    "        # Unsqueeze positional encoding to (1, seq_len, embed_dim) for broadcasting.\n",
    "        # Note: Requires positional_encoding to be on the same device as x.\n",
    "        # Note: This crude addition might have shape issues if batch_size > 1 or input seq len varies.\n",
    "        # Ensure positional encoding is sliced/padded if input sequence length differs from self.sequence_length\n",
    "        seq_len = x.shape[1] # Get actual sequence length from input\n",
    "        device = x.device\n",
    "        pos_enc = self.positional_encoding[:seq_len, :].unsqueeze(0).to(device) # Slice, unsqueeze for batch, move to device\n",
    "        x = x + pos_enc # Add positional information\n",
    "\n",
    "        # 3. Pass through Transformer Blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # 4. Final Linear Layer for Logits\n",
    "        # Shape: (batch_size, sequence_length, embedding_dim) -> (batch_size, sequence_length, vocab_size)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Instantiate Model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MinimalTransformer(vocab_size, embedding_dim, hidden_dim, num_heads, num_layers, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5. Loss Function and Optimizer\n",
    "\n",
    "    * Loss Function: CrossEntropyLoss is suitable for multi-class classification\n",
    "    * (predicting the next token ID from the vocabulary). It combines LogSoftmax and NLLLoss.\n",
    "    * It expects raw logits from the model and target class indices.\n",
    "    * Optimizer: Adam is a popular choice for training deep learning models.\n",
    "    * It adapts the learning rate for each parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Training Loop (Simplified)\n",
    "   * --- !!! Placeholder for Training Data !!! ---\n",
    "   * `train_data` needs to be defined before this loop.\n",
    "   * It should be a sequence (list, tuple, or tensor) of token indices.\n",
    "   * Example: train_data = torch.randint(0, vocab_size, (5000,))\n",
    "   * train_data = ... # Load or generate your training data her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training (Simplified Loop)...\")\n",
    "# Check if train_data exists (add a dummy if not for demonstration)\n",
    "if 'train_data' not in globals():\n",
    "    print(\"Warning: 'train_data' not defined. Using dummy data for demonstration.\")\n",
    "    train_data = torch.randint(0, vocab_size, (1000,)) # Dummy data\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # TODO: Implement proper data loading (DataLoader), batching, and shuffling.\n",
    "    # TODO: Add model.train() at the beginning of the epoch and model.eval() for evaluation.\n",
    "\n",
    "    # Simple iteration through the data in chunks of sequence_length\n",
    "    # This is NOT proper batching; it processes one sequence at a time.\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i in range(0, len(train_data) - sequence_length -1, sequence_length): # Ensure target is within bounds\n",
    "        # a. Prepare Input and Target Sequences\n",
    "        # Input: Sequence of tokens\n",
    "        input_sequence = train_data[i : i + sequence_length]\n",
    "        # Target: Next token for each input token (shifted by one)\n",
    "        target_sequence = train_data[i + 1 : i + sequence_length + 1]\n",
    "\n",
    "        # Convert to tensors (ensure LongTensor for embedding lookup and loss)\n",
    "        # Add a batch dimension (unsqueeze(0)) as the model expects Batch x SeqLen x ...\n",
    "        input_tensor = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0) # Shape: (1, sequence_length)\n",
    "        target_tensor = torch.tensor(target_sequence, dtype=torch.long) # Shape: (sequence_length)\n",
    "\n",
    "        # b. Zero Gradients\n",
    "        # Clear gradients accumulated from the previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # c. Forward Pass\n",
    "        # Get model predictions (logits) for the input sequence.\n",
    "        # Output shape: (1, sequence_length, vocab_size)\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        # d. Calculate Loss\n",
    "        # CrossEntropyLoss expects:\n",
    "        # - Input: (N, C) where C = number of classes (vocab_size)\n",
    "        # - Target: (N) where each value is a class index\n",
    "        # Reshape model output and target tensor accordingly.\n",
    "        # output.view(-1, vocab_size) -> Flattens batch and seq_len dimensions: (1 * sequence_length, vocab_size)\n",
    "        # target_tensor.view(-1) -> Flattens target: (sequence_length)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_tensor.view(-1))\n",
    "\n",
    "        # e. Backward Pass\n",
    "        # Compute gradients of the loss with respect to model parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # f. Update Weights\n",
    "        # Adjust model parameters based on the computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    if num_batches > 0:\n",
    "      avg_loss = epoch_loss / num_batches\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, No batches processed.\")\n",
    "\n",
    "\n",
    "print(\"Training Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row_embeddings(table, tokenizer, embedding_layer, sequence_length, positional_encoding):\n",
    "    \"\"\"\n",
    "    Converts each row of a pandas DataFrame into a sequence of embeddings.\n",
    "\n",
    "    This function iterates through the rows of the input DataFrame, performs\n",
    "    the following steps for each row:\n",
    "    1. Concatenates the string representation of all cell values in the row.\n",
    "    2. Tokenizes the concatenated string using the provided tokenizer.\n",
    "    3. Pads or truncates the resulting token sequence to a specified fixed length.\n",
    "    4. Looks up the embedding vector for each token ID using the embedding layer.\n",
    "    5. Adds pre-computed positional encodings to the token embeddings.\n",
    "    Finally, it stacks the embeddings for all rows into a single batch tensor.\n",
    "\n",
    "    Args:\n",
    "        table (pd.DataFrame): The input pandas DataFrame where each row needs\n",
    "            to be embedded.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): An instance of a Hugging\n",
    "            Face tokenizer (or any object with a compatible `.encode()` method\n",
    "            and `.pad_token_id` attribute) used to convert text to token IDs.\n",
    "        embedding_layer (nn.Embedding): A PyTorch embedding layer instance used\n",
    "            to map token IDs to dense embedding vectors. Its input dimension\n",
    "            should match the tokenizer's vocabulary size, and its output\n",
    "            dimension is the `embedding_dim`.\n",
    "        sequence_length (int): The target fixed length for the token sequences.\n",
    "            Sequences shorter than this will be padded, and longer sequences\n",
    "            will be truncated.\n",
    "        positional_encoding (torch.Tensor): A pre-computed tensor containing\n",
    "            positional encodings. It's expected to have a shape compatible\n",
    "            with `(1, max_sequence_length, embedding_dim)` or be broadcastable.\n",
    "            Only the first `sequence_length` positions will be used.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the embeddings for all rows in the\n",
    "            input table, stacked along the batch dimension. The shape will be\n",
    "            `(num_rows, sequence_length, embedding_dim)`, where `num_rows` is\n",
    "            the number of rows in the input `table`.\n",
    "    \"\"\"\n",
    "    row_embeddings = [] # List to store embeddings for each row\n",
    "    # Ensure positional encoding is on the correct device and sliced correctly later\n",
    "    device = embedding_layer.weight.device\n",
    "    # Slice positional encoding to match embedding dim and move to device\n",
    "    # Assuming positional_encoding has shape (1, max_len, pos_encoding_dim)\n",
    "    # We need (max_len, embedding_dim) before slicing further\n",
    "    pos_enc_prepared = positional_encoding.squeeze(0)[:, :embedding_layer.embedding_dim].to(device)\n",
    "\n",
    "\n",
    "    for index, row in table.iterrows():\n",
    "        # --- Step 1: Data Conversion & Concatenation ---\n",
    "        # Convert all values in the row to strings and join them with spaces.\n",
    "        row_text = \" \".join(str(value) for value in row.values)\n",
    "\n",
    "        # --- Step 2: Tokenization ---\n",
    "        # Convert the text representation into a list of token IDs.\n",
    "        # `add_special_tokens=True` might add tokens like [CLS], [SEP].\n",
    "        tokens = tokenizer.encode(row_text, add_special_tokens=True)\n",
    "\n",
    "        # --- Step 3: Padding/Truncating ---\n",
    "        # Adjust the length of the token ID list to match `sequence_length`.\n",
    "        current_length = len(tokens)\n",
    "        if current_length < sequence_length:\n",
    "            # Pad with the tokenizer's padding token ID if too short.\n",
    "            padding_length = sequence_length - current_length\n",
    "            tokens = tokens + [tokenizer.pad_token_id] * padding_length\n",
    "        elif current_length > sequence_length:\n",
    "            # Truncate if too long.\n",
    "            tokens = tokens[:sequence_length]\n",
    "\n",
    "        # --- Step 4: Embedding Lookup ---\n",
    "        # Convert the list of token IDs into a PyTorch tensor.\n",
    "        token_ids = torch.tensor(tokens, dtype=torch.long).to(device)\n",
    "        # Pass the token IDs through the embedding layer to get dense vectors.\n",
    "        # Shape: (sequence_length) -> (sequence_length, embedding_dim)\n",
    "        embeddings = embedding_layer(token_ids)\n",
    "\n",
    "        # --- Step 5: Positional Encoding ---\n",
    "        # Add positional information to the token embeddings.\n",
    "        # Slices the pre-computed positional encoding to match the sequence length.\n",
    "        # Shape: (sequence_length, embedding_dim) + (sequence_length, embedding_dim)\n",
    "        embeddings = embeddings + pos_enc_prepared[:sequence_length, :]\n",
    "\n",
    "        # Append the final embedding sequence for the current row to the list.\n",
    "        row_embeddings.append(embeddings)\n",
    "\n",
    "    # --- Stacking ---\n",
    "    # Convert the list of row embeddings (each of shape (sequence_length, embedding_dim))\n",
    "    # into a single tensor by stacking them along a new dimension (dimension 0).\n",
    "    # Resulting shape: (num_rows, sequence_length, embedding_dim)\n",
    "    row_embeddings_batch = torch.stack(row_embeddings)\n",
    "    return row_embeddings_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coloum Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_embeddings(table, tokenizer, embedding_layer, sequence_length, positional_encoding, aggregation_method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Generates a single embedding vector for each column in a pandas DataFrame.\n",
    "\n",
    "    This function iterates through the columns of the input DataFrame. For each\n",
    "    column, it performs the following steps:\n",
    "    1. Converts every cell value in the column to its string representation.\n",
    "    2. Tokenizes each cell's string value individually.\n",
    "    3. Pads or truncates each resulting token sequence to a specified fixed length.\n",
    "    4. Looks up the embedding vectors for all tokens in all cells of the column.\n",
    "    5. Aggregates the embeddings associated with the column (e.g., by taking\n",
    "       the mean across all cells and all token positions) to produce a single\n",
    "       vector representation for that column.\n",
    "    Finally, it stacks the aggregated embeddings for all columns into a single tensor.\n",
    "\n",
    "    Args:\n",
    "        table (pd.DataFrame): The input pandas DataFrame.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): An instance of a Hugging\n",
    "            Face tokenizer (or compatible object) used to convert text to token IDs.\n",
    "        embedding_layer (nn.Embedding): A PyTorch embedding layer instance used\n",
    "            to map token IDs to dense embedding vectors.\n",
    "        sequence_length (int): The target fixed length for token sequences *per cell*.\n",
    "            Sequences shorter than this will be padded, longer ones truncated.\n",
    "        positional_encoding (torch.Tensor): A pre-computed tensor containing\n",
    "            positional encodings. Note: In this specific implementation, the\n",
    "            positional encoding is passed as an argument but **is not actually\n",
    "            used** before the aggregation step. Its inclusion might be for\n",
    "            compatibility or future extension. Expected shape is typically\n",
    "            broadcastable like `(1, max_len, embedding_dim)`.\n",
    "        aggregation_method (str, optional): The method used to aggregate the\n",
    "            embeddings of all cells within a column into a single vector.\n",
    "            Currently supports:\n",
    "                - \"mean\": Averages embeddings across all rows and all token\n",
    "                          positions within the column.\n",
    "            Defaults to \"mean\".\n",
    "            # TODO: Implement other aggregation methods like \"sum\", \"max\", etc.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the aggregated embeddings for each column\n",
    "            in the input table, stacked along the first dimension. The shape\n",
    "            will be `(num_columns, embedding_dim)`, where `num_columns` is the\n",
    "            number of columns in the input `table`.\n",
    "    \"\"\"\n",
    "    column_embeddings = [] # List to store the final embedding for each column\n",
    "    device = embedding_layer.weight.device # Use the same device as the embedding layer\n",
    "\n",
    "    for col in table.columns:\n",
    "        # --- Step 1: Data Conversion (Column-wise) ---\n",
    "        # Get all values from the current column and convert them to strings.\n",
    "        column_text = [str(value) for value in table[col].values]\n",
    "\n",
    "        # --- Step 2: Tokenization (Cell-wise within Column) ---\n",
    "        # Tokenize each cell's text individually. Result is a list of lists of token IDs.\n",
    "        tokens = [tokenizer.encode(text, add_special_tokens=True) for text in column_text]\n",
    "\n",
    "        # --- Step 3: Padding/Truncating (Cell-wise within Column) ---\n",
    "        # Ensure each cell's token sequence has the fixed `sequence_length`.\n",
    "        padded_tokens = []\n",
    "        for token_sequence in tokens:\n",
    "            current_length = len(token_sequence)\n",
    "            if current_length < sequence_length:\n",
    "                # Pad if shorter\n",
    "                padding_length = sequence_length - current_length\n",
    "                token_sequence = token_sequence + [tokenizer.pad_token_id] * padding_length\n",
    "            elif current_length > sequence_length:\n",
    "                # Truncate if longer\n",
    "                token_sequence = token_sequence[:sequence_length]\n",
    "            padded_tokens.append(token_sequence)\n",
    "\n",
    "        # --- Step 4: Embedding Lookup (For all cells in Column) ---\n",
    "        # Convert the list of padded token sequences into a single tensor.\n",
    "        # Shape: (num_rows, sequence_length)\n",
    "        token_ids = torch.tensor(padded_tokens, dtype=torch.long).to(device)\n",
    "        # Get embeddings for all tokens in all cells of the column.\n",
    "        # Shape: (num_rows, sequence_length, embedding_dim)\n",
    "        embeddings = embedding_layer(token_ids)\n",
    "\n",
    "        # --- Step 5: Column Aggregation ---\n",
    "        # Aggregate the embeddings tensor to get a single vector for the column.\n",
    "        if aggregation_method == \"mean\":\n",
    "            # Calculate the mean across the row dimension (dim=0) and sequence dimension (dim=1).\n",
    "            # Shape: (num_rows, sequence_length, embedding_dim) -> (embedding_dim)\n",
    "            column_embedding = torch.mean(embeddings, dim=[0, 1])\n",
    "        elif aggregation_method == \"sum\":\n",
    "             # Calculate the sum across the row dimension (dim=0) and sequence dimension (dim=1).\n",
    "             # Shape: (num_rows, sequence_length, embedding_dim) -> (embedding_dim)\n",
    "            column_embedding = torch.sum(embeddings, dim=[0, 1])\n",
    "        # Add more aggregation methods here (e.g., max pooling) if needed\n",
    "        # elif aggregation_method == \"max\":\n",
    "        #     column_embedding = torch.max(embeddings, dim=1)[0] # Max over sequence\n",
    "        #     column_embedding = torch.max(column_embedding, dim=0)[0] # Max over rows - check logic\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregation method: {aggregation_method}\")\n",
    "\n",
    "        # Append the aggregated embedding for the current column to the list.\n",
    "        column_embeddings.append(column_embedding)\n",
    "\n",
    "    # --- Stacking ---\n",
    "    # Stack the aggregated embeddings for all columns along a new dimension (dim 0).\n",
    "    # List of tensors (embedding_dim,) -> Tensor (num_columns, embedding_dim)\n",
    "    column_embeddings_batch = torch.stack(column_embeddings)\n",
    "    return column_embeddings_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Row and Column Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume create_row_embeddings and create_column_embeddings are defined\n",
    "# and have been used to generate the following tensors:\n",
    "\n",
    "# Placeholder for context - these would be generated by previous functions\n",
    "# Example shapes based on previous documentation examples:\n",
    "# table = pd.DataFrame({'col1': [1,2], 'col2': ['a','b']}) # Example: 2 rows, 2 columns\n",
    "# sequence_length = 32\n",
    "# embedding_dim = 64\n",
    "# tokenizer = ... ; embedding_layer = ... ; positional_encoding = ...\n",
    "# row_embeddings = create_row_embeddings(table, tokenizer, embedding_layer, sequence_length, positional_encoding)\n",
    "# # Expected shape: (num_rows, sequence_length, embedding_dim) -> e.g., (2, 32, 64)\n",
    "# column_embeddings = create_column_embeddings(table, tokenizer, embedding_layer, sequence_length, positional_encoding)\n",
    "# # Expected shape: (num_columns, embedding_dim) -> e.g., (2, 64)\n",
    "\n",
    "# --- Start of Documented Code Snippet ---\n",
    "\n",
    "# 1. Obtain Pre-computed Embeddings (Context)\n",
    "# row_embeddings: Tensor containing embeddings for each token sequence per row.\n",
    "#                 Expected Shape: (num_rows, sequence_length, embedding_dim)\n",
    "# column_embeddings: Tensor containing aggregated embeddings for each column.\n",
    "#                    Expected Shape: (num_columns, embedding_dim)\n",
    "\n",
    "# 2. Prepare Column Embeddings for Concatenation\n",
    "# Get the number of rows from the row_embeddings tensor.\n",
    "num_rows = row_embeddings.shape[0]\n",
    "\n",
    "# Reshape and repeat column embeddings to match the row dimension.\n",
    "# a. `column_embeddings.unsqueeze(0)`: Adds a batch dimension at the beginning.\n",
    "#    Shape changes from (num_columns, embedding_dim)\n",
    "#    to (1, num_columns, embedding_dim).\n",
    "# b. `.repeat(num_rows, 1, 1)`: Repeats the tensor `num_rows` times along the\n",
    "#    newly added dimension (dim 0), and keeps dimensions 1 and 2 the same size.\n",
    "#    This effectively creates a copy of the column embeddings for each row.\n",
    "#    Resulting Shape: (num_rows, num_columns, embedding_dim)\n",
    "repeated_column_embeddings = column_embeddings.unsqueeze(0).repeat(num_rows, 1, 1)\n",
    "\n",
    "# 3. Concatenate Row and Repeated Column Embeddings\n",
    "# Combine the row-specific token embeddings with the (now repeated) global column embeddings.\n",
    "# NOTE: The original code concatenates along `dim=-1` (the embedding dimension).\n",
    "# This would require sequence_length == num_columns, which is usually not the case.\n",
    "# The comment `(num_rows, sequence_length + num_columns, embedding_dim)` strongly\n",
    "# suggests the *intended* concatenation dimension is `dim=1` (the sequence dimension).\n",
    "# We will proceed assuming dim=1 was intended for a meaningful combination.\n",
    "\n",
    "# Option A: Concatenating along the sequence dimension (dim=1 - Likely Intended)\n",
    "# This appends the column embedding features *after* the token sequence features for each row.\n",
    "# `row_embeddings` shape:      (num_rows, sequence_length, embedding_dim)\n",
    "# `repeated_column_embeddings` shape: (num_rows, num_columns,     embedding_dim)\n",
    "# Resulting `combined_embeddings` shape: (num_rows, sequence_length + num_columns, embedding_dim)\n",
    "combined_embeddings_dim1 = torch.cat((row_embeddings, repeated_column_embeddings), dim=1)\n",
    "\n",
    "# Option B: Concatenating along the embedding dimension (dim=-1 or dim=2 - As written in original code)\n",
    "# This stacks the embedding vectors themselves. It requires sequence_length == num_columns.\n",
    "# `row_embeddings` shape:      (num_rows, sequence_length, embedding_dim)\n",
    "# `repeated_column_embeddings` shape: (num_rows, num_columns,     embedding_dim)\n",
    "# If sequence_length == num_columns:\n",
    "# Resulting `combined_embeddings` shape: (num_rows, sequence_length, embedding_dim + embedding_dim)\n",
    "# combined_embeddings_dim_neg1 = torch.cat((row_embeddings, repeated_column_embeddings), dim=-1)\n",
    "\n",
    "# Assigning the likely intended result for further use:\n",
    "combined_embeddings = combined_embeddings_dim1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
